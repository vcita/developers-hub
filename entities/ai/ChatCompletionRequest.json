{
  "type": "object",
  "description": "Request body for creating a chat completion. Specify the model, messages, and optional parameters to control the completion behavior.",
  "properties": {
    "model": {
      "type": "string",
      "description": "Model identifier in provider/model format. Supported providers: openai, anthropic, google (e.g., \"openai/gpt-4o\", \"anthropic/claude-sonnet-4-5-20250929\", \"google/gemini-2.5-pro\").",
      "example": "openai/gpt-4o"
    },
    "messages": {
      "type": "array",
      "description": "Array of messages representing the conversation history. Must include at least one message with role 'user'.",
      "items": {
        "$ref": "https://vcita.github.io/developers-hub/entities/ai/ChatMessage.json"
      }
    },
    "stream": {
      "type": "boolean",
      "description": "Whether to stream the response as Server-Sent Events (SSE). When true, the response is delivered incrementally as text/event-stream. Defaults to true.",
      "default": true
    },
    "async": {
      "type": "boolean",
      "description": "Process the request asynchronously. Returns a run UID immediately that can be polled via GET /v3/ai/chat_completion_runs/{uid}. Recommended for thinking models (e.g., openai/o1, openai/o3-mini) that may take longer to respond.",
      "default": false
    },
    "temperature": {
      "type": "number",
      "description": "Sampling temperature between 0 and 2. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.",
      "minimum": 0,
      "maximum": 2,
      "example": 0.7
    },
    "max_tokens": {
      "type": "integer",
      "description": "Maximum number of tokens to generate in the completion. The total length of input tokens and generated tokens is limited by the model's context length.",
      "minimum": 1,
      "example": 1024
    },
    "top_p": {
      "type": "number",
      "description": "Top-p (nucleus) sampling parameter between 0 and 1. The model considers the results of the tokens with top_p probability mass. For example, 0.1 means only tokens comprising the top 10% probability mass are considered.",
      "minimum": 0,
      "maximum": 1,
      "example": 1
    },
    "tools": {
      "type": "array",
      "description": "Array of tool definitions available for the model to call during the completion. The model may choose to call zero or more of these tools based on the conversation context.",
      "items": {
        "$ref": "https://vcita.github.io/developers-hub/entities/ai/ToolDefinition.json"
      }
    },
    "response_format": {
      "$ref": "https://vcita.github.io/developers-hub/entities/ai/ResponseFormat.json"
    },
    "stop": {
      "description": "Up to 4 sequences where the model will stop generating further tokens. The returned text will not contain the stop sequence.",
      "oneOf": [
        { "type": "string" },
        { "type": "array", "items": { "type": "string" } }
      ]
    }
  },
  "required": ["model", "messages"],
  "example": {
    "model": "openai/gpt-4o-mini",
    "messages": [
      { "role": "user", "content": "Say hello" }
    ],
    "stream": false,
    "max_tokens": 100
  }
}
